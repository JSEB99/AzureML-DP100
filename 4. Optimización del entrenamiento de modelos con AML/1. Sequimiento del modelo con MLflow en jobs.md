# Seguimiento del entrenamiento del modelo con MLflow en trabajos

Querrá supervisar el rendimiento del modelo a lo largo del tiempo. Quiere comprender si los nuevos datos cada mes benefician al modelo. Junto al seguimiento de modelos entrenados en cuadernos, también puede usar MLflow para realizar un seguimiento de los modelos en scripts.

**MLflow** es una plataforma de código abierto que le ayuda a realizar un seguimiento de las métricas y artefactos del modelo entre plataformas y se integra con Azure Machine Learning.

Al usar MLflow junto con Azure Machine Learning, puede ejecutar scripts de entrenamiento localmente o en la nube. Puede revisar las métricas y artefactos del modelo en el área de trabajo de Azure Machine Learning para comparar ejecuciones y decidir los pasos siguientes.

## Realización de un seguimiento de métricas con MLflow

Al entrenar un modelo con un script, puede incluir MLflow en los scripts para realizar un seguimiento de los parámetros, las métricas y los artefactos. Al ejecutar el script como un trabajo en Azure Machine Learning, podrá revisar todos los parámetros de entrada y las salidas de cada ejecución.

## Información sobre MLflow

MLflow es una plataforma de código abierto diseñada para administrar el ciclo de vida completo del aprendizaje automático. Como es de código abierto, se puede usar al entrenar modelos en distintas plataformas. Aquí, exploramos cómo podemos integrar MLflow con trabajos de Azure Machine Learning.

Hay dos opciones para realizar un seguimiento de los trabajos de aprendizaje automático con MLflow:

- Habilitación del registro automático mediante `mlflow.autolog()`
- Uso de funciones de registro para realizar un seguimiento de las métricas personalizadas mediante `mlflow.log_*`

Para poder usar cualquiera de estas opciones, debe configurar el entorno para usar MLflow.

## Incluir MLflow en el entorno

Para usar MLflow durante el trabajo de entrenamiento, los paquetes pip `mlflow` y `azureml-mlflow` deben instalarse en el proceso que ejecuta el script. Por lo tanto, debe incluir estos dos paquetes en el entorno. Puede crear un entorno haciendo referencia a un archivo YAML que describe el entorno de Conda. Como parte del entorno de Conda, puede incluir estos dos paquetes.

Por ejemplo, en este entorno personalizado `mlflow` y `azureml-mlflow` se instalan mediante pip:

```YML
name: mlflow-env
channels:
  - conda-forge
dependencies:
  - python=3.8
  - pip
  - pip:
    - numpy
    - pandas
    - scikit-learn
    - matplotlib
    - mlflow
    - azureml-mlflow
```

Una vez definido y registrado el entorno, asegúrese de hacer referencia a él al enviar un trabajo.

## Habilitación del registro automático

Al trabajar con una de las bibliotecas comunes de aprendizaje automático, puede habilitar el registro automático en MLflow. El registro automático anota parámetros, métricas y artefactos de modelo sin que nadie tenga que especificar lo que se debe registrar.

El registro automático es compatible con las bibliotecas siguientes:

- Scikit-learn
- TensorFlow y Keras
- XGBoost
- LightGBM
- Spark
- Fastai
- Pytorch

Para habilitar el registro automático, agregue el código siguiente al script de entrenamiento:

```Python
import mlflow

mlflow.autolog()
```

## Registro de métricas con MLflow

En el script de entrenamiento, puede decidir qué métrica personalizada desea registrar con MLflow.

En función del tipo de valor que quiera registrar, use el comando MLflow para almacenar la métrica con la ejecución del experimento:

- `mlflow.log_param()`: Registre el parámetro de clave-valor único. Use esta función para un parámetro de entrada que desee registrar.
- `mlflow.log_metric()`: Registre una métrica de clave-valor única. El valor debe ser un número. Use esta función para cualquier salida que desee almacenar con la ejecución.
- `mlflow.log_artifact()`: Registre un archivo. Use esta función para cualquier trazado que desee registrar y guárdelo primero como archivo de imagen.

Para agregar MLflow a un script de entrenamiento existente, puede agregar el código siguiente:

```Python
import mlflow

reg_rate = 0.1
mlflow.log_param("Regularization rate", reg_rate)
```

## Enviar archivo

Por último, debe enviar el script de entrenamiento como un trabajo en Azure Machine Learning. Cuando se usa MLflow en un script de entrenamiento y se ejecuta como un trabajo, todos los parámetros, métricas y artefactos a los que se realiza un seguimiento se almacenan con la ejecución del trabajo.

El trabajo se configura como de costumbre. Solo tiene que asegurarse de que el entorno al que hace referencia en el trabajo incluye los paquetes necesarios y el script describe las métricas que desea registrar.

## Visualización de métricas y evaluación de modelos

Después de entrenar y realizar un seguimiento de los modelos con MLflow en Azure Machine Learning, puede explorar las métricas y evaluar dichos modelos.

- Revise las métricas en Estudio de Azure Machine Learning.
- Recupere ejecuciones y métricas con MLflow.

## Visualización de las métricas en Estudio de Azure Machine Learning

Una vez completado el trabajo, puede revisar los parámetros, las métricas y los artefactos registrados en Estudio de Azure Machine Learning.

Al revisar las ejecuciones de trabajos en Estudio de Azure Machine Learning, explorará las métricas de una ejecución de trabajo, que forma parte de un experimento.

Para ver las métricas a través de una interfaz de usuario intuitiva, puede hacer lo siguiente:

1. Para abrir Studio, vaya a https://ml.azure.com.
2. Busque la ejecución del experimento y ábrala para ver sus detalles.
3. En la pestaña Detalles, todos los parámetros registrados se mostrarán en Parámetros.
4. Seleccione la pestaña Métricas y seleccione la que desea explorar.
5. Los trazados registrados como artefactos se pueden encontrar en Imágenes.
6. Los recursos del modelo que se pueden usar para registrar e implementar el modelo se almacenan en la carpeta modelos en Salidas y registros.

## Recuperación de métricas con MLfLow en un cuaderno

Al ejecutar un script de entrenamiento como trabajo en Azure Machine Learning y realizar un seguimiento del entrenamiento del modelo con MLflow, puede consultar las ejecuciones en un cuaderno mediante MLflow. El uso de MLflow en un cuaderno proporciona más control sobre las ejecuciones que desea recuperar para comparar.

Al usar MLflow para consultar las ejecuciones, hará referencia a experimentos y ejecuciones.

## Búsqueda de todos los experimentos

Puede obtener todos los experimentos activos en el área de trabajo mediante MLflow

```Python
experiments = mlflow.search_experiments(max_results=2)
for exp in experiments:
    print(exp.name)
```

Si también quiere recuperar experimentos que haya archivado, incluya la opción `ViewType.ALL`:

```Python
from mlflow.entities import ViewType

experiments = mlflow.search_experiments(view_type=ViewType.ALL)
for exp in experiments:
    print(exp.name)
```

Para recuperar un experimento específico, puede ejecutar:

```Python
exp = mlflow.get_experiment_by_name(experiment_name)
print(exp)
```

> Exploración de la documentación sobre cómo [buscar experimentos con MLflow](https://mlflow.org/docs/latest/search-experiments.html)

## Recuperación de ejecuciones

MLflow permite buscar ejecuciones dentro de cualquier experimento. Necesita el id. del experimento o el nombre del experimento.

Por ejemplo, si desea recuperar las métricas de una ejecución específica:

```Python
mlflow.search_runs(exp.experiment_id)
```

Puede buscar ejecuciones en varios experimentos si es necesario. La búsqueda en varios experimentos puede resultar útil en caso de que quiera comparar ejecuciones del mismo modelo cuando se registra en diferentes experimentos (por personas diferentes o iteraciones de proyectos diferentes).

Puede usar `search_all_experiments=True` si quiere realizar la búsqueda en todos los experimentos del área de trabajo.

De manera predeterminada, los experimentos se ordenan de forma descendente por `start_time`, que es el momento en el que el experimento se encontraba en la cola en Azure Machine Learning. Sin embargo, puede cambiar este valor predeterminado mediante el parámetro `order_by`.

por ejemplo, si desea ordenar por hora de inicio y mostrar los dos últimos resultados:

```Python
mlflow.search_runs(exp.experiment_id, order_by=["start_time DESC"], max_results=2)
```

También puede buscar una ejecución con una combinación específica en los hiperparámetros:

```Python
mlflow.search_runs(
    exp.experiment_id, filter_string="params.num_boost_round='100'", max_results=2
)
```

> Exploración de la documentación sobre cómo [buscar ejecuciones con MLflow](https://mlflow.org/docs/latest/search-runs.html)

## [EJERCICIO](https://microsoftlearning.github.io/mslearn-azure-ml/Instructions/08-Script-mlflow-tracking.html)
