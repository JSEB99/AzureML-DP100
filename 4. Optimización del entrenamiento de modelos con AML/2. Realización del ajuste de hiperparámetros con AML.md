# Realización del ajuste de hiperparámetros con Azure Machine Learning

![hyperparameters](./images/hyperparams.png)

El ajuste de los hiperparámetros se logra mediante el entrenamiento de varios modelos, con el mismo algoritmo y los mismos datos de entrenamiento, pero con distintos valores de hiperparámetros. A continuación, el modelo que resulta de cada ejecución de entrenamiento se evalúa para determinar la métrica de rendimiento para la que desea realizar la optimización (por ejemplo, precisión) y se selecciona el modelo de mejor rendimiento.

En Azure Machine Learning, puede ajustar los hiperparámetros enviando un script como un **trabajo de barrido**. Un trabajo de barrido ejecutará una prueba para cada combinación de hiperparámetros que se va a probar. Cada prueba usa un script de entrenamiento con valores de hiperparámetro parametrizados con el fin de entrenar un modelo, y registra la métrica de rendimiento de destino que logra el modelo entrenado.

## Definición del espacio de búsqueda

El conjunto de valores de hiperparámetro probado durante el ajuste de los hiperparámetros se conoce como **espacio de búsqueda**. La definición del rango de valores posibles que se pueden elegir depende del tipo de hiperparámetro.

## Hiperparámetros discretos

Algunos hiperparámetros requieren valores _discretos_; en otras palabras, debe seleccionar el valor de un conjunto _finito_ determinado de posibilidades. Puede definir un espacio de búsqueda de un parámetro discreto mediante una **opción** de una lista de valores explícitos, que puede definir como una lista de Python (`Choice(values=[10,20,30])`), un **rango** (`Choice(values=range(1,10))`) o un conjunto arbitrario de valores separados por comas (`Choice(values=(30,50,100))`).

También puede seleccionar valores discretos de cualquiera de las distribuciones discretas siguientes:

- `QUniform(min_value, max_value, q)`: devuelve un valor como round(Uniform(min_value, max_value) / q) \* q
- `QLogUniform(min_value, max_value, q)`: devuelve un valor como round(exp(Uniform(min_value, max_value)) / q) \* q
- `QNormal(mu, sigma, q)`: devuelve un valor como round(Normal(mu, sigma) / q) \* q
- `QLogNormal(mu, sigma, q)`: devuelve un valor como round(exp(Normal(mu, sigma)) / q) \* q

## Hiperparámetros continuos

Algunos hiperparámetros son _continuos_; es decir, puede usar cualquier valor a lo largo de una escala, lo que da lugar a un número _infinito_ de posibilidades. Si desea definir un espacio de búsqueda para estos tipos de valor, puede usar cualquiera de los tipos de distribución siguientes:

- `Uniform(min_value, max_value)`: devuelve un valor distribuido uniformemente entre min_value y max_value.
- `LogUniform(min_value, max_value)`: devuelve un valor que se extrae según exp(Uniform(min_value, max_value)) de forma que el logaritmo del valor devuelto se distribuya uniformemente.
- `Normal(mu, sigma)`: devuelve un valor real que se distribuye normalmente con media mu y desviación estándar sigma.
- `LogNormal(mu, sigma)`: devuelve un valor extraído según exp(Normal(mu, sigma)) de forma que el logaritmo del valor devuelto se distribuya normalmente.

## Definición de un espacio de búsqueda

Si desea definir un espacio de búsqueda para el ajuste de hiperparámetros, cree un diccionario con la expresión de parámetro adecuada para cada hiperparámetro con nombre.

Por ejemplo, el espacio de búsqueda siguiente indica que el hiperparámetro `batch_size` puede tener el valor 16, 32 o 64, y el hiperparámetro `learning_rate` puede tener cualquier valor de una distribución normal con una media de 10 y una desviación estándar de 3.

```Python
from azure.ai.ml.sweep import Choice, Normal

command_job_for_sweep = job(
    batch_size=Choice(values=[16, 32, 64]),
    learning_rate=Normal(mu=10, sigma=3),
)
```

## Configuración de un método de muestreo

Los valores específicos que se usan en una ejecución de ajuste de hiperparámetros, o trabajo de barrido, dependen del tipo de muestreo que se usa.

Hay tres métodos de muestreo principales disponibles en Azure Machine Learning:

- **Muestreo de cuadrícula:** prueba todas las combinaciones posibles.
- **Muestreo aleatorio:** elige aleatoriamente los valores del espacio de búsqueda.
  - **Sobol:** agrega un valor de inicialización al muestreo aleatorio para que los resultados sean reproducibles.
- **Muestreo bayesiano:** elige nuevos valores en función de los resultados anteriores.

> Sobol es una variación del muestreo aleatorio

## Muestreo de cuadrícula - Grid Sampling

El muestreo de cuadrícula solo se puede aplicar **cuando todos los hiperparámetros son discretos**, y se usa para probar cada combinación posible de parámetros en el espacio de búsqueda.

Por ejemplo, en el ejemplo de código siguiente, se usa el muestreo de cuadrícula para probar todas las combinaciones posibles de los valores discretos _batch_size_ y _learning_rate_:

```Python
from azure.ai.ml.sweep import Choice

command_job_for_sweep = command_job(
    batch_size=Choice(values=[16, 32, 64]),
    learning_rate=Choice(values=[0.01, 0.1, 1.0]),
)

sweep_job = command_job_for_sweep.sweep(
    sampling_algorithm = "grid",
    ...
)
```

## Muestreo aleatorio

El muestreo aleatorio se usa para seleccionar aleatoriamente un valor para cada hiperparámetro, **que puede ser una combinación de valores discretos y continuos**, tal como se muestra en el ejemplo de código siguiente:

```Python
from azure.ai.ml.sweep import Normal, Uniform

command_job_for_sweep = command_job(
    batch_size=Choice(values=[16, 32, 64]),
    learning_rate=Normal(mu=10, sigma=3),
)

sweep_job = command_job_for_sweep.sweep(
    sampling_algorithm = "random",
    ...
)
```

## Sobol

Es posible que desee poder reproducir un trabajo de barrido de muestreo aleatorio. Si espera que lo haga, puede usar Sobol en su lugar. Sobol es un tipo de muestreo aleatorio que permite usar una inicialización. Al agregar un valor de inicialización, el trabajo de barrido se puede reproducir y la distribución del espacio de búsqueda se distribuye de forma más uniforme.

En el ejemplo de código siguiente se muestra cómo usar Sobol mediante la adición de un valor de inicialización y una regla, y el uso de la clase `RandomSamplingAlgorithm`:

```Python
from azure.ai.ml.sweep import RandomSamplingAlgorithm

sweep_job = command_job_for_sweep.sweep(
    sampling_algorithm = RandomSamplingAlgorithm(seed=123, rule="sobol"),
    ...
)
```

## Muestreo bayesiano

El muestreo bayesiano elige los valores de hiperparámetro basados en el algoritmo de optimización bayesiano, que intenta seleccionar combinaciones de parámetros que mejorarán el rendimiento con respecto a la selección anterior. En el ejemplo de código siguiente se muestra cómo configurar el muestreo bayesiano:

```Python
from azure.ai.ml.sweep import Uniform, Choice

command_job_for_sweep = job(
    batch_size=Choice(values=[16, 32, 64]),
    learning_rate=Uniform(min_value=0.05, max_value=0.1),
)

sweep_job = command_job_for_sweep.sweep(
    sampling_algorithm = "bayesian",
    ...
)
```

> Solo puede utilizar el muestreo bayesiano con las expresiones de parámetros choice, uniform y quniform.

## Configuración de la terminación anticipada

El ajuste de hiperparámetros le ayuda a ajustar el modelo y a seleccionar los valores de hiperparámetros que harán que el modelo funcione mejor.

Sin embargo, encontrar el mejor modelo puede ser una conquista interminable. Siempre debe tener en cuenta si vale la pena el tiempo y el gasto de probar nuevos valores de hiperparámetros para encontrar un modelo que pueda funcionar mejor.

En cada prueba de un trabajo de barrido, se entrena un nuevo modelo con una nueva combinación de valores de hiperparámetros. Si el entrenamiento de un nuevo modelo no da como resultado un modelo significativamente mejor, es posible que quiera detener el trabajo de barrido y usar el modelo que mejor funcionó hasta el momento.

Al configurar un trabajo de barrido en Azure Machine Learning, también puede establecer un número máximo de pruebas. Un enfoque más sofisticado puede ser detener un trabajo de barrido cuando los nuevos modelos no producen resultados significativamente mejores. Para detener un trabajo de barrido en función del rendimiento de los modelos, puede usar una directiva de terminación anticipada.

## Cuándo usar una directiva de terminación anticipada

La conveniencia de utilizar una directiva de terminación anticipada puede depender del espacio de búsqueda y del método de muestreo con el que se trabaje.

Por ejemplo, puede optar por usar un método de muestreo de cuadrícula en un espacio de búsqueda discreto que da como resultado un máximo de seis pruebas. Con seis pruebas, se entrenará un máximo de seis modelos y una directiva de terminación anticipada puede ser innecesaria.

Una directiva de terminación anticipada puede ser especialmente beneficiosa cuando se trabaja con hiperparámetros continuos en el espacio de búsqueda. Los hiperparámetros continuos presentan un número ilimitado de valores posibles entre los que elegir. Lo más probable es que quiera usar una directiva de terminación anticipada al trabajar con hiperparámetros continuos y un método de muestreo aleatorio o bayesiano.

## Configuración de una directiva de terminación anticipada

Hay dos parámetros principales al elegir usar una directiva de terminación anticipada:

- `evaluation_interval`: especifica en qué intervalo desea evaluar la directiva. Cada vez que se registra la métrica principal para una prueba cuenta como un intervalo.
- `delay_evaluation`: especifica cuándo empezar a evaluar la directiva. Este parámetro permite que se completen al menos un mínimo de pruebas sin una directiva de terminación anticipada que les afecte.

Es posible que los nuevos modelos sigan funcionando solo ligeramente mejor que los anteriores. Para determinar la medida en que un modelo debe funcionar mejor que las pruebas anteriores, hay tres opciones para la terminación anticipada:

- **Directiva de ladrón:** usa un `slack_factor` (relativo) o `slack_amount` (absoluto). Cualquier modelo nuevo debe funcionar dentro del rango de holgura del modelo de mejor rendimiento.
- **Directiva de detención de mediana:** usa la mediana de los promedios de la métrica principal. Cualquier nuevo modelo debe funcionar mejor que la mediana.
- **Directiva de selección de truncamiento:** usa un `truncation_percentage`, que es el porcentaje de pruebas con menor rendimiento. Cualquier modelo nuevo debe funcionar mejor que las pruebas de menor rendimiento.

## Directiva de bandidos

Puede usar una directiva de ladrón para detener una prueba si la métrica de rendimiento objetivo es inferior a la mejor prueba hasta el momento por un margen especificado.

Por ejemplo, el siguiente código aplica una directiva de ladrón con un retraso de cinco pruebas, evalúa la directiva en cada intervalo y permite una cantidad de holgura absoluta de 0,2.

```Python
from azure.ai.ml.sweep import BanditPolicy

sweep_job.early_termination = BanditPolicy(
    slack_amount = 0.2,
    delay_evaluation = 5,
    evaluation_interval = 1
)
```

Imagine que la métrica principal es la precisión del modelo. Cuando, después de las cinco primeras pruebas, el modelo que mejor funciona tiene una precisión de 0,9, cualquier modelo nuevo tiene que funcionar mejor que **(0,9-0,2) o 0,7**. Si la precisión del nuevo modelo es superior a 0,7, el trabajo de barrido continuará. Si el nuevo modelo tiene una puntuación de precisión inferior a 0,7, la directiva finalizará el trabajo de barrido.

![bandit-policy](./images/bandit-policy.png)

También puede aplicar una directiva de ladrón mediante un factor de margen de demora, que compara la métrica de rendimiento como una proporción en lugar de un valor absoluto.

## Directiva de mediana de detención

La directiva de detención de la mediana abandona las pruebas cuando la métrica de rendimiento de destino es inferior a la mediana del promedio de ejecuciones para todas las pruebas.

Por ejemplo, el código siguiente aplica una directiva de detención mediana con un retraso de cinco pruebas y evalúa la directiva a cada intervalo.

```Python
from azure.ai.ml.sweep import MedianStoppingPolicy

sweep_job.early_termination = MedianStoppingPolicy(
    delay_evaluation = 5,
    evaluation_interval = 1
)
```

Imagine que la métrica principal es la precisión del modelo. Cuando se registra la precisión para la sexta prueba, la métrica debe ser mayor que la mediana de las puntuaciones de precisión hasta el momento. Supongamos que la mediana de las puntuaciones de precisión hasta ahora es 0,82. `Si la precisión del nuevo modelo es superior a 0,82, el trabajo de barrido continuará`. Si el nuevo modelo tiene una puntuación de precisión inferior a 0,82, la directiva detendrá el trabajo de barrido y no se entrenará ningún modelo nuevo.

![median-stopping](./images/median-stopping.png)

## Directiva de selección de truncamiento

Una directiva de selección de truncamiento cancela el X % de ejecuciones con menor rendimiento en cada intervalo de evaluación en función del valor de truncation_percentage que especificó para X.

Por ejemplo, el código siguiente aplica una directiva de selección de truncamiento con un retraso de cuatro pruebas, evalúa la directiva en cada intervalo y usa un porcentaje de truncamiento del 20 %.

```Python
from azure.ai.ml.sweep import TruncationSelectionPolicy

sweep_job.early_termination = TruncationSelectionPolicy(
    evaluation_interval=1,
    truncation_percentage=20,
    delay_evaluation=4
)
```

Imagine que la métrica principal es la precisión del modelo. Cuando se registra la precisión de la quinta prueba, `la métrica no debe estar en el peor 20 % de las pruebas hasta el momento`. En este caso, el 20 % se traduce en una prueba. En otras palabras, si la quinta prueba no es el modelo con peor rendimiento hasta el momento, el trabajo de barrido continuará. Si la quinta prueba tiene la puntuación de precisión más baja de todos los ensayos hasta el momento, el trabajo de barrido se detendrá.

![truncation-selection](./images/truncation-selection.png)

## Uso de un trabajo de barrido para el ajuste de hiperparámetros

En Azure Machine Learning, puede ajustar los hiperparámetros ejecutando un **trabajo de barrido.**

## Creación de un script de entrenamiento para el ajuste de hiperparámetros

Para ejecutar un trabajo de barrido, debe crear un script de entrenamiento tal como lo haría con cualquier otro experimento de entrenamiento, salvo que el script **debe:**

- Incluir un argumento para cada hiperparámetro que quiere modificar.
- Registrar la métrica de rendimiento de destino con MLflow. Una métrica de registro permite que el trabajo de barrido evalúe el rendimiento de las pruebas que inicia, e identifica la que genera el modelo de mejor rendimiento.

> Aprenda a [realizar un seguimiento de los experimentos y modelos de aprendizaje automático con MLflow en Azure Machine Learning.](https://learn.microsoft.com/es-es/azure/machine-learning/how-to-use-mlflow-cli-runs?view=azureml-api-2&tabs=interactive%2Ccli)

Por ejemplo, el script de ejemplo siguiente entrena un modelo de regresión logística con un argumento `--regularization` para establecer el hiperparámetro de _tasa de regularización_ y registra la métrica de _precisión_ con el nombre `Accuracy`:

```Python
import argparse
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
import mlflow

# get regularization hyperparameter
parser = argparse.ArgumentParser()
parser.add_argument('--regularization', type=float, dest='reg_rate', default=0.01)
args = parser.parse_args()
reg = args.reg_rate

# load the training dataset
data = pd.read_csv("data.csv")

# separate features and labels, and split for training/validatiom
X = data[['feature1','feature2','feature3','feature4']].values
y = data['label'].values
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)

# train a logistic regression model with the reg hyperparameter
model = LogisticRegression(C=1/reg, solver="liblinear").fit(X_train, y_train)

# calculate and log accuracy
y_hat = model.predict(X_test)
acc = np.average(y_hat == y_test)
mlflow.log_metric("Accuracy", acc)
```

## Configuración y ejecución de un trabajo de barrido

Para preparar el trabajo de barrido, primero debe crear un **trabajo de comando** base que especifique qué script ejecutar y defina los parámetros usados por el script:

```Python
from azure.ai.ml import command

# configure command job as base
job = command(
    code="./src",
    command="python train.py --regularization ${{inputs.reg_rate}}",
    inputs={
        "reg_rate": 0.01,
    },
    environment="AzureML-sklearn-0.24-ubuntu18.04-py37-cpu@latest",
    compute="aml-cluster",
    )
```

Después, puede invalidar los parámetros de entrada con el espacio de búsqueda:

```Python
from azure.ai.ml.sweep import Choice

command_job_for_sweep = job(
    reg_rate=Choice(values=[0.01, 0.1, 1]),
)
```

Por último, llame a `sweep()` en el trabajo de comando para barrer el espacio de búsqueda:

```Python
from azure.ai.ml import MLClient

# apply the sweep parameter to obtain the sweep_job
sweep_job = command_job_for_sweep.sweep(
    compute="aml-cluster",
    sampling_algorithm="grid",
    primary_metric="Accuracy",
    goal="Maximize",
)

# set the name of the sweep job experiment
sweep_job.experiment_name="sweep-example"

# define the limits for this sweep
sweep_job.set_limits(max_total_trials=4, max_concurrent_trials=2, timeout=7200)

# submit the sweep
returned_sweep_job = ml_client.create_or_update(sweep_job)
```

## Supervisión y revisión de trabajos de barrido

Puede supervisar los trabajos de barrido en Estudio de Azure Machine Learning. El trabajo de barrido iniciará pruebas para cada combinación de hiperparámetros que se va a probar. Para cada prueba, puede revisar todas las métricas registradas.

Además, puede evaluar y comparar modelos mediante la visualización de las pruebas en Estudio. Puede ajustar cada gráfico para mostrar y comparar los valores de hiperparámetros y las métricas de cada prueba.

> Obtenga más información sobre cómo [visualizar trabajos de ajuste de hiperparámetros.](https://learn.microsoft.com/es-es/azure/machine-learning/how-to-tune-hyperparameters?view=azureml-api-2#visualize-hyperparameter-tuning-jobs?azure-portal=true)

## [EJERCICIO](https://microsoftlearning.github.io/mslearn-azure-ml/Instructions/09-Hyperparameter-tuning.html)
